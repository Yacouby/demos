{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = os.path.abspath('../')\n",
    "data_path = os.path.join(base_path, 'data')\n",
    "features_path = os.path.join(base_path, 'features')\n",
    "artifacts_path = os.path.join(base_path, 'artifacts')\n",
    "\n",
    "os.environ['data_path'] = data_path\n",
    "os.environ['features_path'] = features_path\n",
    "os.environ['artifacts_path'] = artifacts_path\n",
    "os.environ['METRICS_TABLE'] = data_path\n",
    "os.environ['FEATURES_TABLE'] = features_path\n",
    "\n",
    "os.environ['base_dataset'] = artifacts_path + '/selected_features.parquet'\n",
    "os.environ['base_dataset'] = data_path + '/' +os.listdir(data_path)[-1]\n",
    "os.environ['BATCHES_TO_GENERATE'] = '20'\n",
    "os.environ['keys'] = 'timestamp,company,data_center,device'\n",
    "os.environ['metrics'] = '[\"cpu_utilization\", \"throughput\", \"packet_loss\", \"latency\"]'\n",
    "os.environ['metric_aggs'] = '[\"mean\", \"sum\", \"std\", \"var\", \"min\", \"max\", \"median\"]'\n",
    "os.environ['suffix'] = 'daily'\n",
    "os.environ['window'] = '3'\n",
    "os.environ['center'] = '0'\n",
    "os.environ['inplace'] = '0'\n",
    "os.environ['drop_na'] = '1'\n",
    "os.environ['files_to_select'] = '1'\n",
    "os.environ['label_col'] = 'is_error'\n",
    "os.environ['is_save_to_tsdb'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlrun.datastore import DataItem\n",
    "import ast\n",
    "\n",
    "from typing import Union\n",
    "from mlrun import mlconf, import_function, mount_v3io, NewTask, function_to_module, get_or_create_ctx\n",
    "from mlrun.run import get_dataitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tsdb(context):\n",
    "    df = context.v3f.read(backend='tsdb', query=f'select cpu_utilization, latency, packet_loss, throughput, is_error from {context.metrics_table}',\n",
    "                          start=f'now-2h', end='now', multi_index=True)\n",
    "    df = format_df_from_tsdb(context, df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parquet(context):\n",
    "    mpath = [os.path.join(context.metrics_table, file) for file in os.listdir(context.metrics_table) if file.endswith(('parquet', 'pq'))]\n",
    "    files_by_updated = sorted(mpath, key=os.path.getmtime, reverse=True)\n",
    "    context.logger.info(files_by_updated)\n",
    "    latest = files_by_updated[:context.files_to_select]\n",
    "    context.logger.info(f'Aggregating {latest}')\n",
    "    input_df = pd.concat([pd.read_parquet(df) for df in latest])\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tsdb(context, features: pd.DataFrame):   \n",
    "    context.v3f.write('tsdb', context.features_table, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(context, df: pd.DataFrame):\n",
    "    print('Saving features to Parquet')\n",
    "    \n",
    "    # Need to fix timestamps from ns to ms if we write to parquet\n",
    "    df = df.reset_index()\n",
    "    df['timestamp'] = df.loc[:, 'timestamp'].astype('datetime64[ms]')\n",
    "    \n",
    "    # Fix indexes\n",
    "    df = df.set_index(context.keys)\n",
    "    \n",
    "    # Save parquet\n",
    "    first_timestamp = df.index[0][0].strftime('%Y%m%dT%H%M%S')\n",
    "    last_timestamp = df.index[-1][0].strftime('%Y%m%dT%H%M%S')\n",
    "    filename = first_timestamp + '-' + last_timestamp + '.parquet'\n",
    "    filepath = os.path.join(context.features_table, filename)\n",
    "    with open(filepath, 'wb+') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    \n",
    "    mlconf.dbpath = 'http://mlrun-api:8080'\n",
    "    \n",
    "    # Setup aggregate function\n",
    "    aggregate_fn = import_function(os.getenv('aggregate_fn_url', 'hub://aggregate'))\n",
    "    mod = function_to_module(aggregate_fn)\n",
    "    setattr(context, 'aggregate', mod.aggregate)\n",
    "    \n",
    "    ag_context = get_or_create_ctx('aggregate')\n",
    "    setattr(context, 'mlrun_ctx', ag_context)\n",
    "    \n",
    "    # How many batches to create? (-1 will run forever)\n",
    "    batches_to_generate = int(os.getenv('BATCHES_TO_GENERATE', 20))\n",
    "    setattr(context, 'batches_to_generate', batches_to_generate)\n",
    "    setattr(context, 'batches_generated', 0)\n",
    "    \n",
    "    # Set vars from env\n",
    "    setattr(context, 'metrics_table', os.getenv('METRICS_TABLE', 'netops_metrics'))\n",
    "    setattr(context, 'features_table', os.getenv('FEATURES_TABLE', 'netops_features'))\n",
    "    setattr(context, 'keys', os.getenv('keys', '').split(','))\n",
    "    setattr(context, 'metrics', ast.literal_eval(os.getenv('metrics', '')))\n",
    "    setattr(context, 'metric_aggs', ast.literal_eval(os.getenv('metric_aggs', '')))\n",
    "    setattr(context, 'suffix', os.getenv('suffix', '_agg'))\n",
    "    setattr(context, 'window', int(os.getenv('window', '3')))\n",
    "    setattr(context, 'center', bool(int(os.getenv('center', '0'))))\n",
    "    setattr(context, 'inplace', bool(int(os.getenv('inplace', '0'))))\n",
    "    setattr(context, 'drop_na', bool(int(os.getenv('drop_na', '1'))))\n",
    "    setattr(context, 'files_to_select', int(os.getenv('files_to_select', 1)))\n",
    "    \n",
    "    sample_dataset = get_dataitem(os.environ['base_dataset']).as_df()\n",
    "    selected_features = [col for col in list(sample_dataset.columns) if col != os.getenv('label_col', '')]\n",
    "    aggregated_features = [feature.split('_')[:-1] for feature in selected_features if feature.endswith(context.suffix)]\n",
    "    base_features = set([f[0] for f in aggregated_features])\n",
    "    aggregations = set([f[1] for f in aggregated_features])\n",
    "    setattr(context, 'features', selected_features)\n",
    "    setattr(context, 'base_features', base_features)\n",
    "    setattr(context, 'aggregations', aggregations)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save to TSDB\n",
    "    is_save_to_tsdb = bool(int(os.getenv('save_to_tsdb', '0')))\n",
    "    if is_save_to_tsdb:\n",
    "        # Create our DB client\n",
    "        v3io_client = v3f.Client(address='framesd:8081', container='bigdata')\n",
    "        setattr(context, 'v3f', v3io_client)\n",
    "        \n",
    "        # Create features table if neede\n",
    "        context.v3f.create('tsdb', context.features_table, attrs={'rate': '1/s'}, if_exists=1)\n",
    "        \n",
    "        # Set TSDB reading function\n",
    "        setattr(context, 'read', get_data_tsdb)\n",
    "        \n",
    "        # Set TSDB saving function\n",
    "        setattr(context, 'write', save_to_tsdb)\n",
    "        \n",
    "    # Save to Parquet\n",
    "    else:\n",
    "         # Create saving directory if needed\n",
    "        filepath = os.path.join(context.features_table)\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "            \n",
    "        # Set Parquet reading function\n",
    "        setattr(context, 'read', get_data_parquet)\n",
    "        \n",
    "        # Set Parquet saving function\n",
    "        setattr(context, 'write', save_to_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    \n",
    "    # Limit the number of generated batches to save cluster resources\n",
    "    # for people forgetting the demo running\n",
    "    if (context.batches_to_generate == -1) or (context.batches_generated <= context.batches_to_generate):\n",
    "    \n",
    "        # Get latest parquets\n",
    "        df = context.read(context)\n",
    "\n",
    "        # Call aggregate\n",
    "        res = context.aggregate(context=context.mlrun_ctx,\n",
    "                  df_artifact=df,\n",
    "                  save_to=context.features_table, \n",
    "                  keys=context.keys, \n",
    "                  metrics=context.metrics, \n",
    "                  metric_aggs=context.metric_aggs, \n",
    "                  suffix=context.suffix, \n",
    "                  window=context.window, \n",
    "                  center=context.center, \n",
    "                  inplace=context.inplace,\n",
    "                  drop_na=context.drop_na)\n",
    "\n",
    "        context.logger.info(f'res.columns: {res.columns}')\n",
    "        context.logger.info(f'context.columns: {context.features}')\n",
    "        res = res[context.features]\n",
    "\n",
    "        # Save\n",
    "        context.write(context, res)\n",
    "        \n",
    "        # Update batches count\n",
    "        context.batches_generated += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 13:54:07,937 [info] logging run results to: http://mlrun-api:8080\n"
     ]
    }
   ],
   "source": [
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2021-10-03 13:54:09,057 [info] ['/User/test/demos/network-operations/data/20211003T135320-20211003T145320.parquet']\n",
      "Python> 2021-10-03 13:54:09,058 [info] Aggregating ['/User/test/demos/network-operations/data/20211003T135320-20211003T145320.parquet']\n",
      "> 2021-10-03 13:54:09,079 [info] Aggregating from Buffer\n",
      "> 2021-10-03 13:54:09,290 [info] Logging artifact\n",
      "Python> 2021-10-03 13:54:09,292 [info] res.columns: Index(['cpu_utilization', 'cpu_utilization_is_error', 'latency',\n",
      "       'latency_is_error', 'packet_loss', 'packet_loss_is_error', 'throughput',\n",
      "       'throughput_is_error', 'is_error', 'cpu_utilization_mean_daily',\n",
      "       'cpu_utilization_sum_daily', 'cpu_utilization_std_daily',\n",
      "       'cpu_utilization_var_daily', 'cpu_utilization_min_daily',\n",
      "       'cpu_utilization_max_daily', 'cpu_utilization_median_daily',\n",
      "       'throughput_mean_daily', 'throughput_sum_daily', 'throughput_std_daily',\n",
      "       'throughput_var_daily', 'throughput_min_daily', 'throughput_max_daily',\n",
      "       'throughput_median_daily', 'packet_loss_mean_daily',\n",
      "       'packet_loss_sum_daily', 'packet_loss_std_daily',\n",
      "       'packet_loss_var_daily', 'packet_loss_min_daily',\n",
      "       'packet_loss_max_daily', 'packet_loss_median_daily',\n",
      "       'latency_mean_daily', 'latency_sum_daily', 'latency_std_daily',\n",
      "       'latency_var_daily', 'latency_min_daily', 'latency_max_daily',\n",
      "       'latency_median_daily'],\n",
      "      dtype='object')\n",
      "Python> 2021-10-03 13:54:09,292 [info] context.columns: ['cpu_utilization', 'cpu_utilization_is_error', 'latency', 'latency_is_error', 'packet_loss', 'packet_loss_is_error', 'throughput', 'throughput_is_error']\n",
      "Saving features to Parquet\n"
     ]
    }
   ],
   "source": [
    "event = nuclio.Event(body='')\n",
    "out = handler(context, event)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "import os \n",
    "import nuclio\n",
    "\n",
    "base_path = os.path.abspath('../')\n",
    "data_path = os.path.join(base_path, 'data')\n",
    "features_path = os.path.join(base_path, 'features')\n",
    "artifacts_path = os.path.join(base_path, 'artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f39d5f68e50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = code_to_function('nuclio-preprocessor',\n",
    "                      kind='nuclio',\n",
    "                      project='network-operations', image='mlrun/ml-models')\n",
    "\n",
    "fn.apply(mount_v3io())\n",
    "fn.set_envs({'data_path' : data_path,\n",
    "             'features_path' : features_path,\n",
    "             'artifacts_path' : artifacts_path,\n",
    "             'METRICS_TABLE' : data_path,\n",
    "             'FEATURES_TABLE' : features_path,\n",
    "             'base_dataset' : data_path + '/' + os.listdir(data_path)[-1],\n",
    "             'BATCHES_TO_GENERATE' :'20',\n",
    "             'keys' : 'timestamp,company,data_center,device',\n",
    "             'metrics' : '[\"cpu_utilization\", \"throughput\", \"packet_loss\", \"latency\"]',\n",
    "             'metric_aggs' : '[\"mean\", \"sum\", \"std\", \"var\", \"min\", \"max\", \"median\"]',\n",
    "             'suffix' : 'daily',\n",
    "             'window' : '3',\n",
    "             'center' : '0',\n",
    "             'inplace' : '0',\n",
    "             'drop_na' : '1',\n",
    "             'files_to_select' : '1',\n",
    "             'label_col' : 'is_error',\n",
    "             'is_save_to_tsdb' : '0'})\n",
    "fn.add_trigger('cron', nuclio.triggers.CronTrigger(interval='1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 12:36:35,548 [info] function spec saved to path: ../src/preprocessor.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f39d5f68e50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.save()\n",
    "fn.export('../src/preprocessor.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 12:36:35,554 [info] Starting remote function deploy\n",
      "2021-10-03 12:36:35  (info) Deploying function\n",
      "2021-10-03 12:36:35  (info) Building\n",
      "2021-10-03 12:36:35  (info) Staging files and preparing base images\n",
      "2021-10-03 12:36:35  (info) Building processor image\n",
      "2021-10-03 12:36:37  (info) Build complete\n",
      "> 2021-10-03 12:36:43,268 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-network-operations-nuclio-preprocessor.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['default-tenant.app.dev8.lab.iguazeng.com:31499']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://default-tenant.app.dev8.lab.iguazeng.com:31499'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.deploy(project='network-operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 12:36:46,507 [info] invoking function: {'method': 'GET', 'path': 'http://nuclio-network-operations-nuclio-preprocessor.default-tenant.svc.cluster.local:8080/'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.invoke('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
