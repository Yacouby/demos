{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclio - Generator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = os.path.abspath('../')\n",
    "data_path = os.path.join(base_path, 'data')\n",
    "src_path = os.path.join(base_path, 'src')\n",
    "os.environ['data_path'] = data_path\n",
    "os.environ['src_path'] = src_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['METRICS_CONFIGURATION_FILEPATH']= os.path.join(base_path + '/src/metric_configurations.yaml')\n",
    "os.environ['SAVE_TO']= data_path\n",
    "os.environ['BATCHES_TO_GENERATE']= '20'\n",
    "os.environ['SAVE_DEPLOYMENT']='1'\n",
    "os.environ['DEPLOYMENT_TABLE']= 'devices'\n",
    "os.environ['SECS_TO_GENERATE']='3600'\n",
    "os.environ['SAVE_TO_TSDB']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "# DB Connection\n",
    "import v3io_frames as v3f\n",
    "\n",
    "# Data generator\n",
    "from v3io_generator import metrics_generator, deployment_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_deployment():\n",
    "    print('creating deployment')\n",
    "    # Create meta-data factory\n",
    "    dep_gen = deployment_generator.deployment_generator()\n",
    "    faker=dep_gen.get_faker()\n",
    "\n",
    "    # Design meta-data\n",
    "    dep_gen.add_level(name='company',number=2,level_type=faker.company)\n",
    "    dep_gen.add_level('data_center',number=2,level_type=faker.street_name)\n",
    "    dep_gen.add_level('device',number=2,level_type=faker.msisdn)\n",
    "\n",
    "    # Create meta-data\n",
    "    deployment_df = dep_gen.generate_deployment()\n",
    "    return deployment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_deployment_exist(path):\n",
    "    # Checking shared path for the devices table\n",
    "    return os.path.exists(f'/v3io/bigdata/{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_deployment_from_kv(client, path):\n",
    "    print(f'Retrieving deployment from {path}')\n",
    "    context.logger.debug(f'Retrieving deployment from {path}')\n",
    "    # Read the devices table from our KV store\n",
    "    deployment_df = client.read(backend='kv', table=path)\n",
    "    \n",
    "    # Reset index to column\n",
    "    deployment_df.index.name = 'device'\n",
    "    deployment_df = deployment_df.reset_index()\n",
    "    return deployment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_deployment_to_kv(path, df, client=None):\n",
    "    # Save deployment to our KV store\n",
    "    client.write(backend='kv', table='netops_devices',dfs=df, index_cols=['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_deployment(path, save_to_cloud=False, client=None):\n",
    "    if client and _is_deployment_exist(path):\n",
    "        # Get deployment from KV\n",
    "        deployment_df = _get_deployment_from_kv(client, path)\n",
    "    else:\n",
    "        # Create deployment\n",
    "        deployment_df = _create_deployment()\n",
    "        \n",
    "        if client and save_to_cloud:\n",
    "            _save_deployment_to_kv(path, deployment_df, client)\n",
    "\n",
    "    return deployment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indexes(df):\n",
    "    df = df.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_tsdb(context, metrics: pd.DataFrame):\n",
    "    print('Saving metrics to TSDB')\n",
    "    \n",
    "    context.v3f.write('tsdb', context.metrics_table, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_parquet(context, metrics):\n",
    "    print('Saving metrics to Parquet')\n",
    "    df = pd.concat(itertools.chain(metrics))\n",
    "    \n",
    "    # Need to fix timestamps from ns to ms if we write to parquet\n",
    "    df = df.reset_index()\n",
    "    df['timestamp'] = df.loc[:, 'timestamp'].astype('datetime64[ms]')\n",
    "    \n",
    "    # Fix indexes\n",
    "    df = set_indexes(df)\n",
    "    \n",
    "    # Save parquet\n",
    "    first_timestamp = df.index[0][0].strftime('%Y%m%dT%H%M%S')\n",
    "    last_timestamp = df.index[-1][0].strftime('%Y%m%dT%H%M%S')\n",
    "    filename = first_timestamp + '-' + last_timestamp + '.parquet'\n",
    "    print(filename)\n",
    "    filepath = os.path.join(context.metrics_table, filename)\n",
    "    print(filepath)\n",
    "    with open(filepath, 'wb+') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_deployment_initialized(context):\n",
    "    return hasattr(context, 'metric_generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    \n",
    "    # How many batches to create? (-1 will run forever)\n",
    "    batches_to_generate = int(os.getenv('BATCHES_TO_GENERATE', 20))\n",
    "    setattr(context, 'batches_to_generate', batches_to_generate)\n",
    "    setattr(context, 'batches_generated', 0)\n",
    "    \n",
    "    # Get saving configuration\n",
    "    save_to_tsdb = (int(os.getenv('SAVE_TO_TSDB', 1)) == 1)\n",
    "    \n",
    "    # Set metrics table\n",
    "    metrics_table = os.getenv('SAVE_TO', 'netops_metrics')\n",
    "    setattr(context, 'metrics_table', metrics_table) \n",
    "\n",
    "    # TSDB Based demo\n",
    "    if save_to_tsdb:\n",
    "        context.logger.debug('Saving to TSDB')\n",
    "        # Create our DB client\n",
    "        client = v3f.Client(address='framesd:8081', container='bigdata')\n",
    "        \n",
    "        # Create TSDB table if needed\n",
    "        client.create('tsdb', metrics_table, rate='1/s', if_exists=1)\n",
    "        \n",
    "        # Set saving function\n",
    "        setattr(context, 'write', save_metrics_to_tsdb)\n",
    "    \n",
    "    # Parquet based demo\n",
    "    else:\n",
    "        context.logger.debug('Saving to Parquet')\n",
    "        # Set empty client for verification purposes\n",
    "        client = None\n",
    "          \n",
    "        # Create saving directory\n",
    "        filepath = os.path.join(metrics_table)\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "        \n",
    "        # Set saving function\n",
    "        setattr(context, 'write', save_metrics_to_parquet)\n",
    "    \n",
    "          \n",
    "    # Set batch endtime\n",
    "    secs_to_generate = os.getenv('SECS_TO_GENERATE', 10)\n",
    "    setattr(context, 'secs_to_generate', secs_to_generate)\n",
    "    \n",
    "     \n",
    "    \n",
    "    # Generate or create deployment\n",
    "    deployment_df = get_or_create_deployment(os.environ['DEPLOYMENT_TABLE'], os.environ['SAVE_DEPLOYMENT'], client)\n",
    "    # Convert to log_dataset\n",
    "    \n",
    "    deployment_df['cpu_utilization'] = 70\n",
    "    deployment_df['latency'] = 0\n",
    "    deployment_df['packet_loss'] = 0\n",
    "    deployment_df['throughput'] = 290\n",
    "    deployment_df.head()\n",
    "    \n",
    "    # Get metrics configuration\n",
    "    # Move to get-object from store:///\n",
    "    with open(os.getenv('METRICS_CONFIGURATION_FILEPATH', '/src/metrics_configuration.yaml'), 'r') as f:\n",
    "        metrics_configuration = yaml.load(f)\n",
    "        \n",
    "    # Create metrics generator\n",
    "    initial_timestamp = int(os.getenv('initial_timestamp', (datetime.datetime.now()-datetime.timedelta(days=1)).timestamp()))\n",
    "    met_gen = metrics_generator.Generator_df(metrics_configuration, \n",
    "                                             user_hierarchy=deployment_df, \n",
    "                                             initial_timestamp=initial_timestamp)\n",
    "    setattr(context, 'metric_generator', met_gen)\n",
    "    \n",
    "    # Set client\n",
    "    setattr(context, 'v3f', client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    \n",
    "    # Limit the number of generated batches to save cluster resources\n",
    "    # for people forgetting the demo running\n",
    "    if (context.batches_to_generate == -1) or (context.batches_generated <= context.batches_to_generate):\n",
    "        \n",
    "        # Create metrics generator based on YAML configuration and deployment\n",
    "        metrics = context.metric_generator.generate_range(start_time=datetime.datetime.now(),\n",
    "                                         end_time=datetime.datetime.now()+datetime.timedelta(seconds=int(context.secs_to_generate)),\n",
    "                                         as_df=True)\n",
    "\n",
    "        # Save Generated metrics\n",
    "        context.write(context, metrics)\n",
    "        \n",
    "        # Update batches count\n",
    "        context.batches_generated += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /User/test/demos/network-operations/src/metric_configurations.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {os.environ['METRICS_CONFIGURATION_FILEPATH']}\n",
    "errors: {length_in_ticks: 50, rate_in_ticks: 150}\n",
    "timestamps: {interval: 5s, stochastic_interval: true}\n",
    "metrics:\n",
    "  cpu_utilization:\n",
    "    accuracy: 2\n",
    "    distribution: normal\n",
    "    distribution_params: {mu: 70, noise: 0, sigma: 10}\n",
    "    is_threshold_below: true\n",
    "    past_based_value: false\n",
    "    produce_max: false\n",
    "    produce_min: false\n",
    "    validation:\n",
    "      distribution: {max: 1, min: -1, validate: false}\n",
    "      metric: {max: 100, min: 0, validate: true}\n",
    "  latency:\n",
    "    accuracy: 2\n",
    "    distribution: normal\n",
    "    distribution_params: {mu: 0, noise: 0, sigma: 5}\n",
    "    is_threshold_below: true\n",
    "    past_based_value: false\n",
    "    produce_max: false\n",
    "    produce_min: false\n",
    "    validation:\n",
    "      distribution: {max: 1, min: -1, validate: false}\n",
    "      metric: {max: 100, min: 0, validate: true}\n",
    "  packet_loss:\n",
    "    accuracy: 0\n",
    "    distribution: normal\n",
    "    distribution_params: {mu: 0, noise: 0, sigma: 2}\n",
    "    is_threshold_below: true\n",
    "    past_based_value: false\n",
    "    produce_max: false\n",
    "    produce_min: false\n",
    "    validation:\n",
    "      distribution: {max: 1, min: -1, validate: false}\n",
    "      metric: {max: 50, min: 0, validate: true}\n",
    "  throughput:\n",
    "    accuracy: 2\n",
    "    distribution: normal\n",
    "    distribution_params: {mu: 250, noise: 0, sigma: 20}\n",
    "    is_threshold_below: false\n",
    "    past_based_value: false\n",
    "    produce_max: false\n",
    "    produce_min: false\n",
    "    validation:\n",
    "      distribution: {max: 1, min: -1, validate: false}\n",
    "      metric: {max: 300, min: 0, validate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset locally\n",
    "Running this step will generate the base data file for the project.  \n",
    "This will be used as our training dataset later on in the [project notebook](../project.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating deployment\n",
      "Saving metrics to Parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211003T135320-20211003T145320.parquet\n",
      "/User/test/demos/network-operations/data/20211003T135320-20211003T145320.parquet\n"
     ]
    }
   ],
   "source": [
    "# nuclio: ignore\n",
    "init_context(context)\n",
    "event = nuclio.Event(body='')\n",
    "output = handler(context, event)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to cluster\n",
    "(For streaming demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f38979ce250>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "import nuclio\n",
    "import os\n",
    "\n",
    "fn = code_to_function(name='nuclio-generator',\n",
    "                      kind='nuclio', with_doc=False,image='mlrun/ml-models')\n",
    "\n",
    "fn.spec.build.commands = ['pip install pyyaml', 'pip install pytimeparse', 'pip install v3io_frames', 'pip install -i https://test.pypi.org/simple/ v3io-generator',\n",
    "                          'pip install faker']\n",
    "fn.add_trigger('cron', nuclio.triggers.CronTrigger(interval='1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 13:51:23,375 [info] function spec saved to path: ../src/generator.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f38979ce250>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.save()\n",
    "fn.export('../src/generator.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 13:51:23,383 [info] Starting remote function deploy\n",
      "2021-10-03 13:51:23  (info) Deploying function\n",
      "2021-10-03 13:51:23  (info) Building\n",
      "2021-10-03 13:51:23  (info) Staging files and preparing base images\n",
      "2021-10-03 13:51:23  (info) Building processor image\n",
      "2021-10-03 13:51:25  (info) Build complete\n",
      "2021-10-03 13:51:31  (info) Function deploy complete\n",
      "> 2021-10-03 13:51:32,061 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-network-operations-nuclio-generator.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['default-tenant.app.dev8.lab.iguazeng.com:30079']}\n"
     ]
    },
    {
     "ename": "MLRunConflictError",
     "evalue": "409 Client Error: Conflict for url: http://mlrun-api:8080/api/func/network-operations/nuclio-generator?tag=&versioned=False: store function network-operations/nuclio-generator details: {'reason': \"MLRunConflictError('409 Client Error: Conflict for url: http://nuclio-dashboard.default-tenant.svc.cluster.local:8070/api/projects')\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/errors.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(response, message)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: http://mlrun-api:8080/api/func/network-operations/nuclio-generator?tag=&versioned=False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMLRunConflictError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c2e30a103dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0;34m'SECS_TO_GENERATE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m              'SAVE_TO_TSDB':0})\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'network-operations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/runtimes/function.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, dashboard, project, tag, verbose, auth_info, disable_auto_mount)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_record\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversioned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, tag, versioned, refresh)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"saving function: {self.metadata.name}, tag: {tag}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         hash_key = db.store_function(\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversioned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    919\u001b[0m         \u001b[0mhash_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_key\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversioned\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/db/httpdb.py\u001b[0m in \u001b[0;36mstore_function\u001b[0;34m(self, function, name, project, tag, versioned)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"store function {project}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         resp = self.api_call(\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         )\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/db/httpdb.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, method, path, error, params, body, json, headers, timeout)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0merror_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"details: {error_details}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{error} {error_details}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0merror_details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mmlrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mmlrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/errors.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(response, message)\u001b[0m\n\u001b[1;32m     63\u001b[0m             raise STATUS_ERRORS[response.status_code](\n\u001b[1;32m     64\u001b[0m                 \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             ) from exc\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mMLRunHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMLRunConflictError\u001b[0m: 409 Client Error: Conflict for url: http://mlrun-api:8080/api/func/network-operations/nuclio-generator?tag=&versioned=False: store function network-operations/nuclio-generator details: {'reason': \"MLRunConflictError('409 Client Error: Conflict for url: http://nuclio-dashboard.default-tenant.svc.cluster.local:8070/api/projects')\"}"
     ]
    }
   ],
   "source": [
    "fn.apply(mount_v3io())\n",
    "fn.set_envs({'METRICS_CONFIGURATION_FILEPATH': os.path.join(os.path.abspath('../'), 'src') + '/metric_configurations.yaml',\n",
    "             'SAVE_TO': os.path.join(base_path, 'streaming', 'metrics'),\n",
    "             'BATCHES_TO_GENERATE':20,\n",
    "             'SAVE_DEPLOYMENT':1,\n",
    "             'DEPLOYMENT_TABLE': 'devices',\n",
    "             'SECS_TO_GENERATE':3600,\n",
    "             'SAVE_TO_TSDB':0})\n",
    "fn.deploy(project='network-operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.invoke('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
