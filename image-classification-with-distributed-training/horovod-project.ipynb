{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification workflow with distributed training\n",
    "The following example demonstrates an end to end data science workflow for building an image classifier using Tensorflow in a Multi-Node settings.  \n",
    "\n",
    "We use a public Cats & Dogs dataset to train a Tensorflow based model, and then deploy it to a Live endpoint where you can send an http request with an image of cats/dogs and get a response back that identify whether it is a cat or a dog.  \n",
    "\n",
    "We also share some information about running distributed workloads in general, and how you can integrate easily with Horovod.\n",
    "\n",
    "This typical data science workflow comprises of the following:\n",
    "\n",
    "* Download anb label the dataset\n",
    "* Training a model on the images dataset\n",
    "* Deploy a function with the new model in a serving layer\n",
    "* Testing the function\n",
    "\n",
    "Key technologies (all open source):\n",
    "* [Tensorflow-Keras](https://github.com/tensorflow/tensorflow) for training the model\n",
    "* [Horovod](https://github.com/horovod/horovod) for running a distributed training\n",
    "* [MLRun](https://github.com/mlrun/mlrun) for building the functions and tracking experiments\n",
    "* [Nuclio](https://github.com/nuclio/nuclio) function for creating a funciton that runs the model in a serving layer\n",
    "\n",
    "Based on:\n",
    "* https://www.kaggle.com/c/dogs-vs-cats/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running distributed workloads\n",
    "\n",
    "Training a Deep Neural Network is a hard task.  With growing datasets, wider and deeper networks, training our Neural Network can require a lot of resources (CPUs / GPUs / Mem and Time).  \n",
    "\n",
    "There are two main reasons why we would like to distribute our Deep Learning workloads:  \n",
    "\n",
    "1. **Model Parallelism** &mdash; The **Model** is too big to fit a single GPU.  \n",
    "In this case the model contains too many parameters to hold within a single GPU.  \n",
    "To negate this we can use strategies like **Parameter Server** or slicing the model into slices of consecutive layers which we can fit in a single GPU.  \n",
    "Both strategies require **Synchronization** between the layers held on different GPUs / Parameter Server shards.  \n",
    "\n",
    "2. **Data Parallelism** &mdash; The **Dataset** is too big to fit a single GPU.  \n",
    "Using methods like **Stochastic Gradient Descent** we can send batches of data to our models for gradient estimation. This comes at the cost of longer time to converge since the estimated gradient may not fully represent the actual gradient.  \n",
    "To increase the likelihood of estimating the actual gradient we could use bigger batches, by sending small batches to different GPUs running the same Neural Network, calculating the batch gradient and then running a **Synchronization Step** to calculate the average gradient over the batches and update the Neural Networks running on the different GPUs.  \n",
    "\n",
    "\n",
    "> It is important to understand that the act of distribution adds extra **Synchronization Costs** which may vary according to your cluster's configuration.  \n",
    "> <br>\n",
    "> As the gradients and NN needs to be propagated to each GPU in the cluster every epoch (or a number of steps), Networking can become a bottleneck and sometimes different configurations need to be used for optimal performance.  \n",
    "> <br>\n",
    "> **Scaling Efficiency** is the metric used to show by how much each additional GPU should benefit the training process with Horovod showing up to 90% (When running with a well written code and good parameters).\n",
    "\n",
    "![Horovod scaling](https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we distribute our training\n",
    "There are two different cluster configurations (which can be combined) we need to take into account.  \n",
    "- **Multi Node** &mdash; GPUs are distributed over multiple nodes in the cluster.  \n",
    "- **Multi GPU** &mdash; GPUs are within a single Node.  \n",
    "\n",
    "In this demo we show a **Multi Node Multi GPU** &mdash; **Data Parallel** enabled training using Horovod.  \n",
    "However, you should always try and use the best distribution strategy for your use case (due to the added costs of the distribution itself, ability to run in an optimized way on specific hardware or other considerations that may arise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Horovod works?\n",
    "Horovod's primary motivation is to make it easy to take a single-GPU training script and successfully scale it to train across many GPUs in parallel. This has two aspects:\n",
    "\n",
    "- How much modification does one have to make to a program to make it distributed, and how easy is it to run it?\n",
    "- How much faster would it run in distributed mode?\n",
    "\n",
    "Horovod Supports TensorFlow, Keras, PyTorch, and Apache MXNet.\n",
    "\n",
    "in MLRun we use Horovod with MPI in order to create cluster resources and allow for optimized networking.  \n",
    "**Note:** Horovd and MPI may use [NCCL](https://developer.nvidia.com/nccl) when applicable which may require some specific configuration arguments to run optimally.\n",
    "\n",
    "Horovod uses this MPI and NCCL concepts for distributed computation and messaging to quickly and easily synchronize between the different nodes or GPUs.\n",
    "\n",
    "![Ring Allreduce Strategy](https://miro.medium.com/max/700/1*XdMlfmOgPCUG9ZOYLTeP9w.jpeg)\n",
    "\n",
    "Horovod will run your code on all the given nodes (Specific node can be addressed via `hvd.rank()`) while using an `hvd.DistributedOptimizer` wrapper to run the **synchronization cycles** between the copies of your Neural Network running at each node.  \n",
    "\n",
    "**Note:** Since all the copies of your Neural Network must be the same, Your workers will adjust themselves to the rate of the slowest worker (simply by waiting for it to finish the epoch and receive its updates). Thus try not to make a specific worker do a lot of additional work on each epoch (Like a lot of saving, extra calculations, etc...) since this can affect the overall training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we integrate TF2 with Horovod?\n",
    "As it's one of the main motivations, integration is fairly easy and requires only a few steps: ([You can read the full instructions for all the different frameworks on Horovod's documentation website](https://horovod.readthedocs.io/en/stable/tensorflow.html)).  \n",
    "\n",
    "1. Run `hvd.init()`.  \n",
    "2. Pin each GPU to a single process.\n",
    "With the typical setup of one GPU per process, set this to local rank. The first process on the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.  \n",
    "``` gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "```\n",
    "3. Scale the learning rate by the number of workers.  \n",
    "Effective batch size in synchronous distributed training is scaled by the number of workers. An increase in learning rate compensates for the increased batch size.\n",
    "4. Wrap the optimizer in `hvd.DistributedOptimizer`.  \n",
    "The distributed optimizer delegates gradient computation to the original optimizer, averages gradients using allreduce or allgather, and then applies those averaged gradients.  \n",
    "For TensorFlow v2, when using a `tf.GradientTape`, wrap the tape in `hvd.DistributedGradientTape` instead of wrapping the optimizer.\n",
    "1. Broadcast the initial variable states from rank 0 to all other processes.  \n",
    "This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.  \n",
    "For TensorFlow v2, use `hvd.broadcast_variables` after models and optimizers have been initialized.\n",
    "1. Modify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.  \n",
    "For TensorFlow v2, construct a `tf.train.Checkpoint` and only call `checkpoint.save()` when `hvd.rank() == 0`.\n",
    "\n",
    "\n",
    "You can go to [Horovod's Documentation](https://horovod.readthedocs.io/en/stable) to read more about horovod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the project notebook by defining MLRun's environment and our current project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code from /User/test/demos/image-classification-with-distributed-training/src-tfv2 will be used to train on the v3io:///projects/{{run.project}}/artifacts/images given dataset\n"
     ]
    }
   ],
   "source": [
    "import nuclio\n",
    "from os import environ, path\n",
    "from mlrun import mlconf\n",
    "\n",
    "# Define the `dbpath` - Where our `mlrun-api` services is available\n",
    "# the `http://mlrun-api:8080` is auto-defined by `kubernetes service` definition \n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "# Define the artifact path for the current user's home directory\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{environ[\"HOME\"]}/artifacts'\n",
    "\n",
    "# set tensorflow version (v1 or v2)\n",
    "tf_ver = 'v2'\n",
    "\n",
    "# specify paths and artifacts target location\n",
    "code_dir = path.join(path.abspath('./'), 'src-tf' + tf_ver) # Where our source code files are saved\n",
    "images_path = path.join(mlconf.artifact_path, 'images') \n",
    "\n",
    "# Specify the project's name for experiment tracking\n",
    "project_name='cat-vs-dog-classification'\n",
    "\n",
    "print(f'Code from {code_dir} will be used to train on the {images_path} given dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for downloading and labeling images\n",
    "Our first task is to download and extract the dataset to a shared location where all our different nodes could access and run the training process using `Tensorflow`.  \n",
    "\n",
    "In the following code block, we define the function `open_archive` which given our s3 `archive_url` downloads and extracts it to our wanted `images/<train/validation>/<label>` directory structure.  \n",
    "\n",
    "We wrap the code block with our `# nuclio: start-code` and `# nuclio: end-code` annotations to let our MLRun parser know what code blocks are to be taken to our project function.\n",
    "\n",
    "**Note:** sometimes after running pip install you need to restart the jupyer kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlrun import DataItem\n",
    " \n",
    "def open_archive(context, \n",
    "                 archive_url: DataItem,\n",
    "                 target_path,\n",
    "                 refresh=False,\n",
    "                 train_size=0.8):\n",
    "    \"\"\"Open a file/object archive into a target directory\n",
    "    \n",
    "    Currently supports zip and tar.gz\n",
    "    \n",
    "    :param context:      function execution context\n",
    "    :param archive_url:  url of archive file\n",
    "    :param target_path:  file system path to store extracted files\n",
    "    :param key:          key of archive contents in artifact store\n",
    "    :param test_size:    set the train dataset size out of total dataset\n",
    "    \"\"\"\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    \n",
    "    # get the archive as a local file (download if needed)\n",
    "    archive_url = archive_url.local()\n",
    "    \n",
    "    context.logger.info('Extracting zip')\n",
    "    extraction_path = os.path.join(target_path, 'tmp')\n",
    "    zip_ref = zipfile.ZipFile(archive_url, 'r')\n",
    "    zip_ref.extractall(extraction_path)\n",
    "    \n",
    "    # get all files paths from `extraction_path`\n",
    "    filenames = [file for file in glob(extraction_path + '/*/*') if file.endswith('.jpg')]\n",
    "    \n",
    "    # extract labels from filenames by their naming convention (<label>.<number>.jpg)\n",
    "    # and calculate how many images we have per label\n",
    "    _extract_label = lambda filename: os.path.basename(filename).split('.')[0]\n",
    "    file_labels = [_extract_label(file) for file in filenames]\n",
    "    labels, label_counts = np.unique(file_labels, return_counts=True)\n",
    "    \n",
    "    # Order the files into a {<label>: [<files list>]} dictionary\n",
    "    files = {label: [] for label in labels}\n",
    "    for label, file in zip(file_labels, filenames):\n",
    "        files[label].append(file)\n",
    "    \n",
    "    # Infer training dataset absolute size\n",
    "    num_files = len(filenames)\n",
    "    num_train = int(np.ceil(num_files * train_size))\n",
    "    num_samples_per_class = int(np.ceil(num_train / len(labels)))\n",
    "    \n",
    "    # create directories for train and validation\n",
    "    for label in labels:\n",
    "        train_dir = os.path.join(target_path, \"train\", label)\n",
    "        validation_dir = os.path.join(target_path, \"validation\", label)\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(validation_dir, exist_ok=True)\n",
    "    \n",
    "    # move the files to their appropriate folders (<train/validation>/<label>/<file>)\n",
    "    # we split the dataset by taking the first `num_samples_per_class` files from each\n",
    "    # label and move them to `train` folder, and to `validation` after.\n",
    "    _extract_detatset = lambda i, per_class: 'train' if i <= per_class else 'validation'\n",
    "    for label, filenames in files.items():\n",
    "        for i, file in enumerate(filenames):\n",
    "            shutil.move(file, os.path.join(target_path, _extract_detatset(i, num_samples_per_class), label, os.path.basename(file)))\n",
    "    shutil.rmtree(extraction_path)\n",
    "\n",
    "    # Add function logging\n",
    "    context.logger.info(f'extracted archive to {target_path}')\n",
    "    context.logger.info(f'Dataset contains the labels {labels}')\n",
    "    \n",
    "    # Log the dataset folder as `content` artifact for later use\n",
    "    context.log_artifact('content', target_path=target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test locally, Download and extract image archive\n",
    "After writing the function, we would like to test it locally first, as part of our development cycle.  \n",
    "For that we can use MLRun's `run_local` which will run the function locally as if it was deployed remotely on the cluster, with all the related tracking. \n",
    "We then define a `NewTask` with the `open_archive` function handler and the needed parameters and run it.  \n",
    "\n",
    "**Note:** The dataset is taken from the Iguazio-sample bucket in S3 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to change the location of the source data, set the `SAMPLE_DATA_SOURCE_URL_PREFIX` environment variable.\n",
    "\n",
    "For example, set it to `/v3io/projects/demos-data/iguazio/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-08-08 05:09:04,339 [info] starting run download uid=e86b8e736a75424c834843cbc4e24b05 DB=http://mlrun-api:8080\n",
      "> 2021-08-08 05:09:04,436 [info] downloading https://s3.wasabisys.com/iguazio/data/image-classification/catsndogs.zip to local tmp\n",
      "> 2021-08-08 05:09:16,681 [info] Extracting zip\n",
      "> 2021-08-08 05:09:28,671 [info] extracted archive to /User/test/demos/image-classification-with-distributed-training/images\n",
      "> 2021-08-08 05:09:28,672 [info] Dataset contains the labels ['cat' 'dog']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cat-vs-dog-classification</td>\n",
       "      <td><div title=\"e86b8e736a75424c834843cbc4e24b05\"><a href=\"https://dashboard.default-tenant.app.dev8.lab.iguazeng.com/mlprojects/cat-vs-dog-classification/jobs/monitor/e86b8e736a75424c834843cbc4e24b05/overview\" target=\"_blank\" >...c4e24b05</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 08 05:09:04</td>\n",
       "      <td>completed</td>\n",
       "      <td>download</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=dani</div><div class=\"dictlist\">kind=handler</div><div class=\"dictlist\">owner=dani</div><div class=\"dictlist\">host=jupyter-dani-86575fbc89-vddq8</div></td>\n",
       "      <td><div title=\"https://s3.wasabisys.com/iguazio/data/image-classification/catsndogs.zip\">archive_url</div></td>\n",
       "      <td><div class=\"dictlist\">target_path=/User/test/demos/image-classification-with-distributed-training/images</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/test/demos/image-classification-with-distributed-training/images\">content</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result5667314b-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result5667314b-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result5667314b\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result5667314b-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run e86b8e736a75424c834843cbc4e24b05 --project cat-vs-dog-classification , !mlrun logs e86b8e736a75424c834843cbc4e24b05 --project cat-vs-dog-classification\n",
      "> 2021-08-08 05:09:28,774 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "# download images from s3 using the local `open_archive` function\n",
    "from mlrun import NewTask, run_local\n",
    "import os\n",
    "\n",
    "# Set the source-data URL\n",
    "url_prefix = os.environ.get('SAMPLE_DATA_SOURCE_URL_PREFIX', 'https://s3.wasabisys.com/iguazio/')\n",
    "\n",
    "open_archive_task = NewTask(name='download', \n",
    "                            handler=open_archive, \n",
    "                            params={'target_path': os.path.abspath('./images')},\n",
    "                            inputs={'archive_url': f'{url_prefix.rstrip(\"/\")}/data/image-classification/catsndogs.zip'})\n",
    "\n",
    "\n",
    "download_run = run_local(open_archive_task, \n",
    "                         project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Data-Science Pipeline with MLRun and Kubeflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a multi-stage project (ingest, label, train, deploy model)\n",
    "\n",
    "Projects are used to package multiple functions, workflows, and artifacts. We usually store project code and definitions in a Git archive.\n",
    "\n",
    "The following code creates a new project in a local dir and initialize git tracking on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project, code_to_function\n",
    "project_dir = './'\n",
    "hvdproj = new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add our `utils` function to the project\n",
    "After we locally testing our function, we convert our inline (notebook) code to a function object and register that under our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = code_to_function(kind='job', \n",
    "                         name='utils',\n",
    "                         image='mlrun/mlrun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f9ae9454910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvdproj.set_function(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a new function for distributed Training with TensorFlow, Keras and Horovod\n",
    "\n",
    "Here we use the same structure as before to deploy our **[cats vs. dogs tensorflow model training file](./src-tfv2/horovod-training.py)** to run on the defined horovod cluster in a distributed manner.  \n",
    "\n",
    "In this demo, we apply Transfer Learning from a `ResNet50V2` pre-trained on imagenet model, with an added custom classification layer:\n",
    "```\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(128, activation=\"relu\", kernel_initializer=\"he_uniform\")(flat1)\n",
    "output = Dense(1, activation=\"sigmoid\")(class1)\n",
    "```\n",
    "We also apply basic Tensorflow Dataset optimisations such as `prefatch` for faster training times.\n",
    "\n",
    "To create a distributed training function, simply:  \n",
    "\n",
    "1. Define the input parameters for the training function.  \n",
    "2. Set the function's `kind='mpijob'` to let MLRun know to apply the job to the MPI CRD and create the requested horovod cluster.  \n",
    "3. Set the number of workers for the horovod cluster to use by setting `trainer.spec.replicas = 4` (default is 1 replica).  \n",
    "\n",
    "#### To run training using GPUs\n",
    "To provide GPU support for our workers we need to edit the following:\n",
    "1. Set the function image to a CUDA enabled image (Required) with GPU versions of the frameworks (if needed - TF 1.x gpu version for example)\n",
    "2. Set the number of GPUs **each worker** will receive by setting `trainer.gpus(1)` (default is 0 GPUs).\n",
    "\n",
    "> You can change `use_gpu` to `True` to enable GPU support with 1 gpu/worker  \n",
    "\n",
    "> Please verify that the `HOROVOD_FILE` path is available from the cluster (Local path and Mounted path may vary)\n",
    "\n",
    "The code in [horovod-training.py](./src-tfv2/horovod-training.py) is already written with Horovod and TF2 guidelines as listed in [Horovod's Tensorflow integration guidelines](https://horovod.readthedocs.io/en/stable/tensorflow.html) and you can inspect it as an example.\n",
    "\n",
    "> Please verify that the HOROVOD_FILE path is available from the cluster (Local path and Mounted path may vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/User/test/demos/image-classification-with-distributed-training/src-tfv2/horovod-training.py\n"
     ]
    }
   ],
   "source": [
    "HOROVOD_FILE = os.path.join(code_dir, 'horovod-training.py')\n",
    "print(HOROVOD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.mpijob.v1.MpiRuntimeV1 at 0x7f9ae1365390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import new_function\n",
    "import os\n",
    "\n",
    "# Set `use_gpu` to True to run the function using a GPU Configuration\n",
    "use_gpu = False\n",
    "\n",
    "image = lambda gpu: 'mlrun/ml-models-gpu' if gpu else 'mlrun/ml-models' \n",
    "\n",
    "# Set basic function parameters\n",
    "trainer = new_function(name='trainer',\n",
    "                       kind='mpijob',\n",
    "                       command=HOROVOD_FILE)\n",
    "trainer.spec.replicas = 2\n",
    "\n",
    "# Set a minimal number of dedicated CPUs per node\n",
    "trainer.with_requests(cpu=4)\n",
    "\n",
    "# Pick image by wanted TF version\n",
    "if tf_ver == 'v1':\n",
    "    trainer.spec.image = f'{image(use_gpu)}:{mlconf.version}-py36'\n",
    "else:\n",
    "    trainer.spec.image = image(use_gpu)\n",
    "    \n",
    "# Add GPUs to workers?\n",
    "if use_gpu:\n",
    "    trainer.gpus(1)\n",
    "\n",
    "# Registre the function to the project\n",
    "hvdproj.set_function(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a serving function from the functions hub (marketplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf_ver == 'v1':\n",
    "    hvdproj.set_function('hub://tf1_serving', 'serving')\n",
    "else:\n",
    "    hvdproj.set_function('hub://tf2_serving', 'serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the source images directory as a project artifact (can be accessed by name)\n",
    "By registering the s3 dataset zip file as a project artifact, we are both registering it in the MLRun DB (which will show it in the UI).  \n",
    "But mainly, we enable access to the artifact through our `store://` annotation.  \n",
    "\n",
    "Through the `store://` we can request specific artifacts via `store://<project>/<artifact>` annotation.  \n",
    "This enables us to set a specific artifact, like the latest dataset, Version and Update it, while keeping the same reference through the code.  \n",
    "Specific versions or artifact tags can be accessed by adding `#<hash>` or `:<tag>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.artifacts.base.Artifact at 0x7f9ae1389dd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvdproj.log_artifact(\n",
    "    'images', \n",
    "    target_path=f'{url_prefix}data/image-classification/catsndogs.zip',\n",
    "    artifact_path=mlconf.artifact_path)\n",
    "#print(hvdproj.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and save a pipeline \n",
    "\n",
    "The following workflow definition will be written into a file, it describes an execution graph (DAG) and how functions are conncted to form an end to end pipline. \n",
    "\n",
    "* Download the images \n",
    "* Label the images (Cats & Dogs)\n",
    "* Train the model using distributed TensorFlow (Horovod)\n",
    "* Deploy the model into a serverless function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    '''\n",
    "    This function will run before running the project.\n",
    "    It allows us to add our specific system configurations to the functions\n",
    "    like mounts or secrets if needed.\n",
    "\n",
    "    In this case we will add Iguazio's user mount to our functions using the\n",
    "    `mount_v3io()` function to automatically set the mount with the needed\n",
    "    variables taken from the environment. \n",
    "    * mount_v3io can be replaced with mlrun.platforms.mount_pvc() for \n",
    "    non-iguazio mount\n",
    "\n",
    "    @param functions: <function_name: function_yaml> dict of functions in the\n",
    "                        workflow\n",
    "    @param project: project object\n",
    "    @param secrets: secrets required for the functions for s3 connections and\n",
    "                    such\n",
    "    '''\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())                  # On Iguazio (Auto-mount /User)\n",
    "        # f.apply(mlrun.platforms.mount_pvc()) # Non-Iguazio mount\n",
    "        \n",
    "    functions['serving'].set_env('MODEL_CLASS', 'TFModel')\n",
    "    functions['serving'].set_env('IMAGE_HEIGHT', '224')\n",
    "    functions['serving'].set_env('IMAGE_WIDTH', '224')\n",
    "    functions['serving'].set_env('ENABLE_EXPLAINER', 'False')\n",
    "    functions['serving'].spec.min_replicas = 1\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Image classification demo',\n",
    "    description='Train an Image Classification TF Algorithm using MLRun'\n",
    ")\n",
    "def kfpipeline(\n",
    "        image_archive='store:///images',\n",
    "        images_dir='/User/artifacts/images',\n",
    "        checkpoints_dir='/User/artifacts/models/checkpoints',\n",
    "        model_name='cat_vs_dog_tfv1',\n",
    "        epochs: int=2):\n",
    "\n",
    "    # step 1: download and prep images\n",
    "    open_archive = funcs['utils'].as_step(name='download',\n",
    "                                          handler='open_archive',\n",
    "                                          params={'target_path': images_dir},\n",
    "                                          inputs={'archive_url': image_archive},\n",
    "                                          outputs=['content'])\n",
    "\n",
    "    # step 2: train the model\n",
    "    train_dir = str(open_archive.outputs['content']) + '/train'\n",
    "    val_dir = str(open_archive.outputs['content']) + '/validation'\n",
    "    train = funcs['trainer'].as_step(name='train',\n",
    "                                     params={'epochs': epochs,\n",
    "                                             'checkpoints_dir': checkpoints_dir,\n",
    "                                             'model_dir'     : 'tfmodels',\n",
    "                                             'train_path'     : train_dir,\n",
    "                                             'val_path'       : val_dir,\n",
    "                                             'batch_size'     : 32},\n",
    "                                     outputs=['model'])\n",
    "\n",
    "    # deploy the model using nuclio functions\n",
    "    deploy = funcs['serving'].deploy_step(models={model_name: train.outputs['model']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.set_workflow('main', 'workflow.py', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run-pipeline'></a>\n",
    "## Run a pipeline workflow\n",
    "You can check the **[workflow.py](./workflow.py)** file to see how functions objects are initialized and used (by name) inside the workflow.\n",
    "The `workflow.py` file has two parts, initialize the function objects and define pipeline dsl (connect the function inputs and outputs).\n",
    "\n",
    "> Note the pipeline can include CI steps like building container images and deploying models.\n",
    "\n",
    "\n",
    "\n",
    "### Run\n",
    "use the `run` method to execute a workflow, you can provide alternative arguments and specify the default target for workflow artifacts.<br>\n",
    "The workflow ID is returned and can be used to track the progress or you can use the hyperlinks\n",
    "\n",
    "> Note: The same command can be issued through CLI commands:<br>\n",
    "    `mlrun project my-proj/ -r main -p \"v3io:///users/admin/mlrun/kfp/{{workflow.uid}}/\"`\n",
    "\n",
    "The dirty flag allow us to run a project with uncommited changes (when the notebook is in the same git dir it will always be dirty)\n",
    "\n",
    "In this cell we will run the `main` workflow via `KubeFlow Pipelines` on top of our cluster.  \n",
    "Running the pipeline may take some time. Due to possible jupyter timeout, it's best to track the pipeline's progress via KFP or the MLRun UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-08-08 05:09:36,046 [info] using in-cluster config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.dev8.lab.iguazeng.com/pipelines/#/experiments/details/0df92102-2260-474c-bac3-6bff455ca40c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.dev8.lab.iguazeng.com/pipelines/#/runs/details/05b50975-85e7-42b5-ae10-8e17e1999656\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-08-08 05:09:36,311 [info] Pipeline run id=05b50975-85e7-42b5-ae10-8e17e1999656, check UI or DB for progress\n",
      "> 2021-08-08 05:09:36,312 [info] waiting for pipeline run completion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-9e2e04d7fa4b>\", line 7, in <module>\n",
      "    dirty=True, watch=True)\n",
      "  File \"/User/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/projects/project.py\", line 1342, in run\n",
      "    self.get_run_status(run, notifiers=RunNotifications(with_slack=True))\n",
      "  File \"/User/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/projects/project.py\", line 1380, in get_run_status\n",
      "    workflow_id, timeout=timeout, expected_statuses=expected_statuses\n",
      "  File \"/User/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/run.py\", line 971, in wait_for_pipeline_completion\n",
      "    resp = client.wait_for_run_completion(run_id, timeout)\n",
      "  File \"/conda/lib/python3.7/site-packages/kfp/_client.py\", line 699, in wait_for_run_completion\n",
      "    time.sleep(5)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/conda/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/conda/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9e2e04d7fa4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     dirty=True, watch=True)\n\u001b[0m",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/projects/project.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, name, workflow_path, arguments, artifact_path, namespace, sync, watch, dirty, ttl)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotifiers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRunNotifications\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_slack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/projects/project.py\u001b[0m in \u001b[0;36mget_run_status\u001b[0;34m(self, workflow_id, timeout, expected_statuses, notifiers)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             run_info = wait_for_pipeline_completion(\n\u001b[0;32m-> 1380\u001b[0;31m                 \u001b[0mworkflow_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_statuses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             )\n",
      "\u001b[0;32m~/.pythonlibs/jupyter-dani/lib/python3.7/site-packages/mlrun/run.py\u001b[0m in \u001b[0;36mwait_for_pipeline_completion\u001b[0;34m(run_id, timeout, expected_statuses, namespace)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_run_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mwait_for_run_completion\u001b[0;34m(self, run_id, timeout)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_run_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "artifact_path = path.abspath('./pipe/{{workflow.uid}}')\n",
    "run_id = hvdproj.run(\n",
    "    'main',\n",
    "    arguments={'model_name': 'cat_vs_dog_tf' + tf_ver,\n",
    "               'images_dir': artifact_path + '/images'}, \n",
    "    artifact_path=artifact_path, \n",
    "    dirty=True, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the serving function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function has been deployed we can test it as a regular REST Endpoint using `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing event\n",
    "cat_image_url = f'{url_prefix}data/image-classification/cat.102.jpg'\n",
    "\n",
    "if 'http' in cat_image_url:\n",
    "    response = requests.get(cat_image_url)\n",
    "    cat_image = response.content\n",
    "    img = Image.open(BytesIO(cat_image))\n",
    "else:\n",
    "    cat_image = open(cat_image_url,'rb').read()\n",
    "    img = Image.open(cat_image_url)\n",
    "\n",
    "print('Test image:')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test The Serving Function (with Image URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = 'http://nuclio-{}-{}:8080'.format(hvdproj.name, hvdproj.func('serving').metadata.name)\n",
    "\n",
    "headers = {'Content-type': 'image/jpeg'}\n",
    "url = addr + f'/cat_vs_dog_tf{tf_ver}/predict'\n",
    "\n",
    "response = requests.post(url=url, \n",
    "                         data=json.dumps({'data_url': cat_image_url}), \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "requests.post(url=url, \n",
    "              data=json.dumps({'data_url': cat_image_url}), \n",
    "              headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test The Serving Function (with Jpeg Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=url, \n",
    "                         data=cat_image, \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "requests.post(url=url, \n",
    "              data=cat_image, \n",
    "              headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
