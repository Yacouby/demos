{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape news and Analyse sentiments\n",
    "This notebook shows an example of scraping news articles linked to specific traded companies and utilizing our predeployed sentiment analysis model server to predict the sentiment of the author towards said companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the nuclio-jupyter package is not installed run !pip install nuclio-jupyter\n",
    "import nuclio \n",
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio env -c V3IO_ACCESS_KEY=${V3IO_ACCESS_KEY}\n",
    "%nuclio env -c V3IO_USERNAME=${V3IO_USERNAME}\n",
    "%nuclio env -c V3IO_API=${V3IO_API}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install beautifulsoup4\n",
    "pip install pandas\n",
    "pip install v3io_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'nuclio'\n",
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-models'\n"
     ]
    }
   ],
   "source": [
    "%%nuclio config \n",
    "kind = \"nuclio\"\n",
    "spec.build.baseImage = \"mlrun/mlrun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import pandas as pd\n",
    "import v3io_frames as v3f\n",
    "from unicodedata import normalize\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import mlrun.feature_store as fs\n",
    "import mlrun\n",
    "from mlrun.datastore.targets import ParquetTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_news_page(stock_string):\n",
    "    request = Request('https://www.investing.com/equities/' + stock_string + '-news', headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    content = urlopen(request).read()\n",
    "    return bs(content, 'html.parser')\n",
    "\n",
    "def get_internal_article_links(page):\n",
    "    news = page.find_all('div', attrs={'class': 'mediumTitle1'})[1]\n",
    "    articles = news.find_all('article', attrs={'class': 'js-article-item articleItem'})\n",
    "    return ['https://www.investing.com' + a.find('a').attrs['href'] for a in articles]\n",
    "\n",
    "def get_article_page(article_link):\n",
    "    request = Request(article_link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    content = urlopen(request).read()\n",
    "    return bs(content, 'html.parser')\n",
    "\n",
    "def clean_paragraph(paragraph):\n",
    "    paragraph = re.sub(r'\\(http\\S+', '', paragraph)\n",
    "    paragraph = re.sub(r'\\([A-Z]+:[A-Z]+\\)', '', paragraph)\n",
    "    paragraph = re.sub(r'[\\n\\t\\s\\']', ' ', paragraph)\n",
    "    return normalize('NFKD', paragraph)    \n",
    "\n",
    "def extract_text(article_page):\n",
    "    text_tag = article_page.find('div', attrs={'class': 'WYSIWYG articlePage'})\n",
    "    paragraphs = text_tag.find_all('p')\n",
    "    text = '\\n'.join([clean_paragraph(p.get_text()) for p in paragraphs[:-1]])\n",
    "    return text\n",
    "\n",
    "import json\n",
    "def get_publish_time(article):\n",
    "    tag = article.find('script',{\"type\" : \"application/ld+json\"}).contents[0]\n",
    "    tag_dict = json.loads(str(tag))\n",
    "    dateModified = tag_dict[\"dateModified\"]\n",
    "    return datetime.strftime(datetime.strptime(dateModified, '%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_score(paragraph_scores):\n",
    "    return sum([score - 1 for score in paragraph_scores['outputs']]) / len(paragraph_scores)\n",
    "\n",
    "def get_article_scores(context, articles, endpoint):\n",
    "    scores = [] \n",
    "    for i, article in enumerate(articles):\n",
    "        context.logger.info(f'getting score for article {i + 1}\\\\{len(articles)}')\n",
    "        event_data = {'inputs': article.split('\\n')}\n",
    "        resp = requests.put(endpoint, json=json.dumps(event_data))\n",
    "        scores.append(get_score(json.loads(resp.text)))\n",
    "    return scores\n",
    "\n",
    "def construct_dataframe(sentiments, items,times):\n",
    "    tickers = [x[0] for x in items]\n",
    "    stock_sent = pd.DataFrame({\"symbol\": tickers, \"sentiment\": sentiments, \"last_reaction\": times})\n",
    "    return stock_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    context.logger.info(\"init news reader context\")\n",
    "    setattr(context, 'PROJECT_NAME', os.getenv('PROJECT_NAME', 'stocks-' + os.getenv('V3IO_USERNAME')))\n",
    "    mlrun.set_environment(project = context.PROJECT_NAME)\n",
    "    \n",
    "    # Declaring feature set\n",
    "    stocks_sent_set = fs.FeatureSet(\"news\", entities=[fs.Entity(\"symbol\")],timestamp_key=\"Datetime\")\n",
    "    stocks_sent_set.set_targets(targets=[ParquetTarget(name=\"news\",partitioned=True,time_partitioning_granularity=\"day\")]\n",
    "                                ,with_defaults=False)\n",
    "    setattr(context, 'stock_feature_set', stocks_sent_set)\n",
    "    \n",
    "#     # Add aggregation \n",
    "#     context.stock_feature_set.add_aggregation(\"Sentiments\",\"Sentiment\",[\"min\",\"max\"],[\"1d\"])\n",
    "    \n",
    "    # saving timestamps to know what to ingest\n",
    "    last_trade_times = {}\n",
    "    setattr(context, 'last_trade_times', last_trade_times)\n",
    "    \n",
    "    v3io_framesd = os.getenv('V3IO_FRAMESD', 'framesd:8081')\n",
    "    token = os.getenv('TOKEN', '')\n",
    "    client = v3f.Client(v3io_framesd, container=os.getenv('V3IO_CONTAINER', 'users'), token=token)\n",
    "    setattr(context, 'v3c', client)\n",
    "\n",
    "    setattr(context, 'stocks_stream', os.getenv('STOCKS_STREAM', os.getenv('V3IO_USERNAME') + '/stocks/stocks_stream'))\n",
    "    context.v3c.create(backend='stream', table=context.stocks_stream, if_exists=1)\n",
    "\n",
    "    setattr(context, 'stocks_tsdb', os.getenv('STOCKS_TSDB_TABLE', os.getenv('V3IO_USERNAME') + '/stocks/stocks_tsdb'))\n",
    "    context.v3c.create(backend='tsdb', table=context.stocks_tsdb, rate='1/s', if_exists=1)\n",
    "\n",
    "    setattr(context, 'sentiment_model_endpoint',\n",
    "            os.getenv('SENTIMENT_MODEL_ENDPOINT', '')) # in the '' should be the model endpoint\n",
    "    context.logger.info(f\"set sentiment_model_endpoint {context.sentiment_model_endpoint}\")\n",
    "    sym_to_url = {'GOOGL': 'google-inc', 'MSFT': 'microsoft-corp', 'AMZN': 'amazon-com-inc',\n",
    "                  'AAPL': 'apple-computer-inc', 'INTC' : 'intel-corp'}\n",
    "    setattr(context, 'sym_to_url', sym_to_url)\n",
    "    setattr(context, 'stocks_kv', os.getenv('STOCKS_KV', os.getenv('V3IO_USERNAME') + '/stocks/stocks_kv'))\n",
    "    context.logger.info('end init context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    context.logger.info(f'Getting news about {context.sym_to_url}')\n",
    "    syms = []\n",
    "    contents = []\n",
    "    links = []\n",
    "    times = []\n",
    "    sentiments = []\n",
    "    all_records = []\n",
    "    for sym, url_string in list(context.sym_to_url.items())[:2]:\n",
    "        context.logger.info(f'Getting news about {sym}')\n",
    "        news_page = get_stock_news_page(url_string)\n",
    "        article_links = get_internal_article_links(news_page)\n",
    "        article_pages = [get_article_page(link) for link in article_links]\n",
    "        articles = [extract_text(article_page) for article_page in article_pages]\n",
    "        curr_sentiments = [0 for article in articles]#get_article_scores(context, articles, context.sentiment_model_endpoint)\n",
    "        curr_times = [get_publish_time(article_page) for article_page in article_pages]\n",
    "        sentiments += curr_sentiments\n",
    "        times += curr_times\n",
    "        time = datetime.strptime(curr_times[0],\"%Y-%m-%d %H:%M:%S\")\n",
    "        last = context.last_trade_times.get(sym)\n",
    "        if not last:\n",
    "            last = datetime(1990,1,1)\n",
    "#         if time > last:\n",
    "        for article, link, sentiment, time in zip(articles, article_links, curr_sentiments, curr_times):\n",
    "            record = {\n",
    "                'content': article,\n",
    "                'time': time,\n",
    "                'symbol': sym,\n",
    "                'link': link,\n",
    "                'sentiment': sentiment\n",
    "            }\n",
    "            context.v3c.execute('stream', context.stocks_stream, 'put', args={'data': json.dumps(record)})\n",
    "            timestamped_record = record.copy()\n",
    "            timestamped_record[\"time\"] = datetime.strptime(record[\"time\"],\"%Y-%m-%d %H:%M:%S\")\n",
    "            if(last):\n",
    "                if(timestamped_record.get(\"time\")>last):\n",
    "                    all_records.append(timestamped_record)\n",
    "                    context.last_trade_times[sym] = time\n",
    "            else:\n",
    "                all_records.append(timestamped_record)\n",
    "                context.last_trade_times[sym] = time\n",
    "            syms.append(sym)\n",
    "            contents.append(article)\n",
    "            links.append(link)\n",
    "            \n",
    "        context.v3c.execute('kv', context.stocks_kv, command='update', args={'key': sym,\n",
    "                                                                         'expression': f\"SET sentiment='{sentiments[-1]}';last_reaction='{times[-1]}'\"})\n",
    "    all_records = pd.DataFrame(all_records)\n",
    "    if(all_records.shape[0]>0):\n",
    "        all_records.columns = [\"Content\",\"Datetime\",\"symbol\",\"Link\",\"Sentiment\"]\n",
    "        # Localizing the datetime\n",
    "        all_records[\"Datetime\"] = all_records[\"Datetime\"].dt.tz_localize('UTC')\n",
    "        # writing to featureset\n",
    "        context.logger.info(f\"Writing new dataframe with shape {all_records.shape} to feature store\")\n",
    "        fs.ingest(context.stock_feature_set, all_records, infer_options=fs.InferOptions.default())\n",
    "    else:\n",
    "        context.logger.info(\"No new data to ingest\")\n",
    "\n",
    "    if len(sentiments) > 0:\n",
    "        df = pd.DataFrame.from_dict({'sentiment': sentiments,\n",
    "                                     'time': times,\n",
    "                                     'symbol': syms})\n",
    "        df = df.set_index(['time', 'symbol'])\n",
    "        df.index = df.index.set_levels([pd.to_datetime(df.index.levels[0]), df.index.levels[1]])\n",
    "        df = df.sort_index(level=0, axis=0)\n",
    "        context.v3c.write(backend='tsdb', table=context.stocks_tsdb, dfs=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2021-06-23 11:12:02,020 [info] init news reader context\n",
      "Python> 2021-06-23 11:12:02,030 [info] set sentiment_model_endpoint \n",
      "Python> 2021-06-23 11:12:02,030 [info] end init context\n"
     ]
    }
   ],
   "source": [
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuclio import Event\n",
    "event = Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2021-06-23 11:12:03,109 [info] Getting news about {'GOOGL': 'google-inc', 'MSFT': 'microsoft-corp', 'AMZN': 'amazon-com-inc', 'AAPL': 'apple-computer-inc', 'INTC': 'intel-corp'}\n",
      "Python> 2021-06-23 11:12:03,111 [info] Getting news about GOOGL\n",
      "Python> 2021-06-23 11:12:14,267 [info] Getting news about MSFT\n",
      "Python> 2021-06-23 11:12:23,896 [info] Writing new dataframe with shape (14, 5) to feature store\n"
     ]
    }
   ],
   "source": [
    "s = handler(context, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-06-28 08:15:12,869 [info] function spec saved to path: 02-read-news.yaml\n"
     ]
    }
   ],
   "source": [
    "from mlrun import code_to_function\n",
    "import os\n",
    "# Export bare function\n",
    "fn = code_to_function('read-news',\n",
    "                      handler='handler')\n",
    "fn.export('02-read-news.yaml')\n",
    "\n",
    "# Set parameters for current deployment\n",
    "fn.add_trigger('cron', nuclio.triggers.CronTrigger('10s'))\n",
    "fn.set_envs({'V3IO_CONTAINER': 'users',\n",
    "             'STOCKS_STREAM': os.getenv('V3IO_USERNAME') + '/stocks/stocks_stream',\n",
    "             'STOCKS_TSDB_TABLE': os.getenv('V3IO_USERNAME') + '/stocks/stocks_tsdb',\n",
    "             'SENTIMENT_MODEL_ENDPOINT': 'http://default-tenant.app.dev8.lab.iguazeng.com:31772',\n",
    "             'PROJECT_NAME' :\"stocks-\" + os.getenv('V3IO_USERNAME')})\n",
    "fn.spec.max_replicas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-06-28 08:15:12,879 [info] Starting remote function deploy\n",
      "2021-06-28 08:15:13  (info) Deploying function\n",
      "2021-06-28 08:15:13  (info) Building\n",
      "2021-06-28 08:15:13  (info) Staging files and preparing base images\n",
      "2021-06-28 08:15:13  (info) Building processor image\n",
      "2021-06-28 08:15:14  (info) Build complete\n",
      "2021-06-28 08:15:22  (info) Function deploy complete\n",
      "> 2021-06-28 08:15:23,728 [info] function deployed, address=default-tenant.app.vmdev31.lab.iguazeng.com:31845\n"
     ]
    }
   ],
   "source": [
    "addr = fn.deploy(project=\"stocks-\" + os.getenv('V3IO_USERNAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {addr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
