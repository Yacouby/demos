{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-mlrun-install\"></a>The notebook runs the face recognition pipeline flow. \n",
    "the flow order is\n",
    "1. encode image: encode the images using open cv into numbers vector\n",
    "2. train: train the model based on the images encoded in step before and save model into mlrun artifacts\n",
    "3. deploy the nuclio face recognition function from nuclio-face-prediction-notebook to predict person based on model deployed\n",
    "4. deploy the api-serving function from nuclio-api-serving notebook to serve images sent from client and return response to clients \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-mlrun-install\"></a>The tutorial uses MLRun to create a project, implement and execute an ML pipeline, and track the execution.\n",
    "(For more information about MLRun, see Step 1.)\n",
    "To use MLRun, you must first ensure that it's installed and running as a service on your platform cluster.\n",
    "Look for an `mlrun` service on the **Services** page of the platform dashboard.\n",
    "For more information and additional assistance, contact the Iguazio [support team](mailto:support@iguazio.com).\n",
    "\n",
    "To use MLRun from Jupyter Notebook, you need to run the following code to install the `mlrun` Python package.\n",
    "This needs to be done only once per Jupyter Notebook service.\n",
    "> **Note:** You must **restart the Jupyter kernel** to complete the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: mlrun\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import IPython\n",
    "\n",
    "required = {'mlrun'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "previously_installed = required.intersection(installed)\n",
    "\n",
    "if missing:\n",
    "    print(f'Installing {\",\".join(missing)}')\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "    print('Restarting kernel')\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "if previously_installed:\n",
    "    print(f'Already installed: {\",\".join(previously_installed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration below is shared across the notebooks. Change the values in this subsection if you would like different configuration settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects in the platform are used to package multiple functions, workflows, and artifacts. Set here the project base name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_BASE_NAME = \"faces\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data in the platform is stored in user-defined data containers. In this case we use the predefined \"users\" container. For more information refer to [Data containers, collections, and objects documentation](https://www.iguazio.com/docs/latest-release/concepts/containers-collections-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:faces/notebooks/face-recognition.ipynb
    "CONTAINER = 'users'\n",
    "WEB_API = \"http://v3io-webapi:8081\""
=======
    "DATA_PATH = '/User/demos/demos/realtime-face-recognition/dataset/'\n",
    "ARTIFACTS_PATH = '/User/demos/demos/realtime-face-recognition/artifacts/'\n",
    "MODELS_PATH = '/User/demos/demos/realtime-face-recognition/models.py'"
>>>>>>> upstream/development:realtime-face-recognition/notebooks/face-recognition.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv, path\n",
    "\n",
    "V3IO_USERNAME = getenv('V3IO_USERNAME')\n",
    "DATA_PATH = path.join(V3IO_USERNAME, 'examples',PROJECT_BASE_NAME, 'data')\n",
    "ARTIFACTS_PATH = path.join(V3IO_USERNAME, 'examples',PROJECT_BASE_NAME, 'artifacts')\n",
    "USER_ARTIFACTS_PATH = path.join('/User', 'examples',PROJECT_BASE_NAME, 'artifacts')\n",
    "FUNCTIONS_PATH=path.abspath('./functions')\n",
    "MODELS_PATH = path.join(FUNCTIONS_PATH, 'models.py')\n",
    "MODEL_PATH=path.join(USER_ARTIFACTS_PATH, 'model.bst')\n",
    "CLASSES_MAP=path.join(ARTIFACTS_PATH, 'idx2name.csv')\n",
    "USER_NAME = getenv('V3IO_USERNAME')\n",
    "ENCODINGS_PATH = path.join(ARTIFACTS_PATH,'encodings') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the mlrun project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/test_faces/demos/faces/notebooks\n",
      "Project name: faces-avia\n"
     ]
    }
   ],
   "source": [
    "from mlrun import new_project\n",
    "\n",
    "project_name = '-'.join(filter(None, [PROJECT_BASE_NAME, getenv('V3IO_USERNAME', None)]))\n",
    "project_path = path.abspath('./')\n",
    "project = new_project(project_name, project_path)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts path: avia/examples/faces/artifacts\n",
      "MLRun DB path: http://mlrun-api:8080\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD:faces/notebooks/face-recognition.ipynb
    "from mlrun import mlconf\n",
    "\n",
    "# Target location for storing pipeline artifacts\n",
    "project.artifact_path = ARTIFACTS_PATH\n",
    "# MLRun DB path or API service URL\n",
    "\n",
    "print(f'Artifacts path: {project.artifact_path}\\nMLRun DB path: {mlconf.dbpath}')"
=======
    "def encode_images(context, cuda=True):\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    context.logger.info(f'Running on device: {device}')\n",
    "    \n",
    "    client = v3f.Client(\"framesd:8081\", container=\"users\")\n",
    "    \n",
    "    if not os.path.exists(DATA_PATH + 'processed'):\n",
    "        os.makedirs(DATA_PATH + 'processed')\n",
    "    \n",
    "    if not os.path.exists(DATA_PATH + 'label_pending'):\n",
    "        os.makedirs(DATA_PATH + 'label_pending')\n",
    "    \n",
    "    # If no train images exist in the predefined path we will train the model on a small dataset of movie actresses\n",
    "    if not os.path.exists(DATA_PATH + 'input'):\n",
    "        os.makedirs(DATA_PATH + 'input')\n",
    "        resp = urlopen('https://iguazio-public.s3.amazonaws.com/roy-actresses/Actresses.zip')\n",
    "        zip_ref = zipfile.ZipFile(BytesIO(resp.read()), 'r')\n",
    "        zip_ref.extractall(DATA_PATH + 'input')\n",
    "        zip_ref.close()\n",
    "    \n",
    "    if os.path.exists(DATA_PATH + 'input/__MACOSX'):\n",
    "        shutil.rmtree(DATA_PATH + 'input/__MACOSX')\n",
    "    \n",
    "    idx_file_path = ARTIFACTS_PATH+\"idx2name.csv\"\n",
    "    if os.path.exists(idx_file_path):\n",
    "        idx2name_df = pd.read_csv(idx_file_path)\n",
    "    else:\n",
    "        idx2name_df = pd.DataFrame(columns=['value', 'name'])\n",
    "    \n",
    "    #creates a mapping of classes(person's names) to target value\n",
    "    new_classes_names = [f for f in os.listdir(DATA_PATH + 'input') if not '.ipynb' in f and f not in idx2name_df['name'].values]\n",
    "    \n",
    "    initial_len = len(idx2name_df)\n",
    "    final_len = len(idx2name_df) + len(new_classes_names)\n",
    "    for i in range(initial_len, final_len):\n",
    "        idx2name_df.loc[i] = {'value': i, 'name': new_classes_names.pop()}\n",
    "    \n",
    "    name2idx = idx2name_df.set_index('name')['value'].to_dict()\n",
    "    \n",
    "    #log name to index mapping into mlrun context\n",
    "    context.log_artifact(TableArtifact('idx2name', df=idx2name_df), target_path='idx2name.csv')\n",
    "    \n",
    "    #generates a list of paths to labeled images \n",
    "    imagePaths = [f for f in paths.list_images(DATA_PATH + 'input') if not '.ipynb' in f]\n",
    "    knownEncodings = []\n",
    "    knownLabels = []\n",
    "    fileNames = []\n",
    "    urls = []\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "        #extracts label (person's name) of the image\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        #prepares to relocate image after extracting features\n",
    "        file_name = imagePath.split(os.path.sep)[-1]\n",
    "        new_path = DATA_PATH + 'processed/' + file_name\n",
    "        \n",
    "        #converts image format to RGB for comptability with face_recognition library\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #detects coordinates of faces bounding boxes\n",
    "        boxes = face_recognition.face_locations(rgb, model='hog')\n",
    "        \n",
    "        #computes embeddings for detected faces\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        \n",
    "        #this code assumes that a person's folder in the dataset does not contain an image with a face other then his own\n",
    "        for enc in encodings:\n",
    "            file_name = name + '_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))                                                           \n",
    "            knownEncodings.append(enc)\n",
    "            knownLabels.append([name2idx[name]])\n",
    "            fileNames.append(file_name)\n",
    "            urls.append(new_path)\n",
    "        \n",
    "        #move image to processed images directory\n",
    "        shutil.move(imagePath, new_path)\n",
    "        \n",
    "    #saves computed encodings to avoid repeating computations\n",
    "    df_x = pd.DataFrame(knownEncodings, columns=['c' + str(i).zfill(3) for i in range(128)]).reset_index(drop=True)\n",
    "    df_y = pd.DataFrame(knownLabels, columns=['label']).reset_index(drop=True)\n",
    "    df_details = pd.DataFrame([['initial training']*3]*len(df_x), columns=['imgUrl', 'camera', 'time'])\n",
    "    df_details['time'] = [datetime.datetime.utcnow()]*len(df_x)\n",
    "    df_details['imgUrl'] = urls\n",
    "    data_df = pd.concat([df_x, df_y, df_details], axis=1)\n",
    "    data_df['fileName'] = fileNames\n",
    "    \n",
    "    client.write(backend='kv', table='iguazio/demos/demos/realtime-face-recognition/artifacts/encodings', dfs=data_df, index_cols=['fileName'])\n",
    "    \n",
    "    with open('encodings_path.txt', 'w+') as f:\n",
    "        f.write('iguazio/demos/demos/realtime-face-recognition/artifacts/encodings')\n",
    "    context.log_artifact('encodings_path', src_path=f.name, target_path=f.name)\n",
    "    os.remove('encodings_path.txt')"
>>>>>>> upstream/development:realtime-face-recognition/notebooks/face-recognition.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the configuration defined in this notebook in the project `params`. We will use these values in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.params['PROJECT_BASE_NAME'] = PROJECT_BASE_NAME\n",
    "project.params['CONTAINER'] = CONTAINER\n",
    "project.params['WEB_API'] = WEB_API\n",
    "project.params['DATA_PATH'] = DATA_PATH\n",
    "project.params['ENCODINGS_PATH'] = ENCODINGS_PATH\n",
    "project.params['MODELS_PATH'] = MODELS_PATH\n",
    "project.params['MODEL_PATH'] = MODEL_PATH\n",
    "project.params['ARTIFACTS_PATH'] = ARTIFACTS_PATH\n",
    "project.params['USER_ARTIFACTS_PATH'] = USER_ARTIFACTS_PATH\n",
    "project.params['CLASSES_MAP'] = CLASSES_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### declare encode-images function for encoding initial images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import mount_v3io, code_to_function\n",
    "encode_images_func = code_to_function('encode-images', kind='job', filename='functions/encode_images.py',image='aviaigz/faces:0.6.0')\n",
    "encode_images_func.deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### declare train function for training the encoded images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import mount_v3io, code_to_function\n",
    "train_func = code_to_function('train', kind='job', filename='functions/train.py',image='aviaigz/faces:0.6.0')\n",
    "train_func.deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### declare face-prediction nuclio function for predicting face based on model created in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio\n",
    "import os\n",
    "from mlrun import mount_v3io, code_to_function\n",
    "nuclio_face_prediction_func = code_to_function('nuclio-face-prediction', kind='nuclio', filename='nuclio-face-prediction.ipynb')\n",
    "# set the API/trigger, attach the home dir to the function\n",
    "nuclio_face_prediction_func.with_http(workers=2).apply(mount_v3io())\n",
    "\n",
    "# set environment variables\n",
    "nuclio_face_prediction_func.set_env('MODELS_PATH', MODELS_PATH)\n",
    "nuclio_face_prediction_func.set_env('MODEL_PATH', MODEL_PATH)\n",
    "nuclio_face_prediction_func.set_env('CLASSES_MAP', CLASSES_MAP)\n",
    "nuclio_face_prediction_func.set_env('V3IO_ACCESS_KEY', os.environ['V3IO_ACCESS_KEY'])\n",
    "nuclio_face_prediction_func.spec.build.base_image = 'mlrun/ml-models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### declare nuclio api-serving function for managing images requests and process face-prediction response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio\n",
    "import os\n",
    "from mlrun import mount_v3io, code_to_function\n",
    "nuclio_api_serving_func = code_to_function('nuclio-api-serving', kind='nuclio', filename='nuclio-api-serving.ipynb')\n",
    "# set the API/trigger, attach the home dir to the function\n",
    "nuclio_api_serving_func.with_http(workers=2).apply(mount_v3io())\n",
    "\n",
    "# set environment variables\n",
    "nuclio_api_serving_func.set_env('DATA_PATH' ,DATA_PATH)\n",
    "nuclio_api_serving_func.set_env('V3IO_ACCESS_KEY', os.environ['V3IO_ACCESS_KEY'])\n",
    "nuclio_api_serving_func.spec.build.base_image = 'mlrun/ml-models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mount_v3io, code_to_function\n",
    "\n",
    "\n",
    "project.set_function(encode_images_func,name='encode-images')\n",
    "project.set_function(train_func,name = 'train')\n",
    "project.set_function(nuclio_face_prediction_func,name = 'nuclio-face-prediction')\n",
    "project.set_function(nuclio_api_serving_func,name = 'nuclio-api-serving')\n",
    "\n",
    "project.func('encode-images').apply(mount_v3io())\n",
    "project.func('train').apply(mount_v3io())\n",
    "project.func('nuclio-face-prediction').apply(mount_v3io())\n",
    "project.func('nuclio-api-serving').apply(mount_v3io())\n",
    "\n",
    "\n",
    "project.func('encode-images').set_env('PYTHONPATH', project_path)\n",
    "project.func('train').set_env('PYTHONPATH', project_path)\n",
    "project.func('nuclio-face-prediction').set_env('PYTHONPATH', project_path)\n",
    "project.func('nuclio-api-serving').set_env('PYTHONPATH', project_path)\n",
    "\n",
    "\n",
    "project.func('encode-images').spec.artifact_path = ARTIFACTS_PATH\n",
    "project.func('train').spec.artifact_path = ARTIFACTS_PATH\n",
    "project.func('nuclio-face-prediction').spec.artifact_path = ARTIFACTS_PATH\n",
    "project.func('nuclio-api-serving').spec.artifact_path = ARTIFACTS_PATH\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-step-create-n-run-ml-pipeline\"></a>\n",
    "## Create and Run a Fully Automated ML Pipeline\n",
    "\n",
    "You're now ready to create a full ML pipeline.\n",
    "This is done by using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/), which is integrated into the Iguazio Data Science Platform.\n",
    "Kubeflow Pipelines is an open-source framework for building and deploying portable, scalable machine-learning workflows based on Docker containers.\n",
    "MLRun leverages this framework to take your existing code and deploy it as steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /User/test_faces/demos/faces/notebooks/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {path.join(project_path, 'workflow.py')}\n",
    "\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io, load_project\n",
    "from os import getenv, path\n",
    "\n",
    "project_path = path.abspath('./')\n",
    "project = load_project(project_path)\n",
    "\n",
    "DATA_PATH =project.params.get('DATA_PATH')\n",
    "USER_ARTIFACTS_PATH = project.params.get('USER_ARTIFACTS_PATH')\n",
    "ARTIFACTS_PATH = project.params.get('ARTIFACTS_PATH')\n",
    "\n",
    "MODELS_PATH = project.params.get('MODELS_PATH')\n",
    "FRAMES_URL = 'framesd:8081'\n",
    "V3IO_ACCESS_KEY = getenv('V3IO_ACCESS_KEY')\n",
    "WEB_API = \"http://v3io-webapi:8081\"\n",
    "ENCODINGS_PATH = project.params.get('ENCODINGS_PATH')\n",
    "\n",
    "funcs = {}\n",
    "project_path = path.abspath('./')\n",
    "faces_params = {'data_path' : DATA_PATH,\n",
    "                'artifacts_path': USER_ARTIFACTS_PATH,\n",
    "                'models_path': MODELS_PATH,\n",
    "                'frames_url': FRAMES_URL,\n",
    "                'token' : V3IO_ACCESS_KEY, \n",
    "                'encodings_path': ENCODINGS_PATH }\n",
    "\n",
    "# Configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    project_path = path.abspath('./')\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())\n",
    "        f.set_env('PYTHONPATH', project_path)\n",
    "        f.spec.artifact_path = ARTIFACTS_PATH\n",
    "        \n",
    "        \n",
    "        \n",
    "# Create a Kubeflow Pipelines pipeline\n",
    "@dsl.pipeline(\n",
    "    name = \"faces-pipeline\",\n",
    "    description = \"faces demo pipeline\"\n",
    ")\n",
    "def kfpipeline():\n",
    "    # encode images\n",
    "    encode = funcs['encode-images'].as_step(\n",
    "        name=\"encode_images\",\n",
    "        params=faces_params,\n",
    "        outputs=['encode']\n",
    "    )\n",
    "    \n",
    "    # train the model based on the images\n",
    "    train = funcs['train'].as_step(\n",
    "        name=\"train\",\n",
    "        params = faces_params,\n",
    "        inputs={'table': encode.outputs},                       \n",
    "        outputs=['training']\n",
    "    )\n",
    "    # deploy the model as nuclio function\n",
    "    nuclio_face_prediction = funcs['nuclio-face-prediction'].deploy_step(                \n",
    "        models={\"nuclio-face-prediction\": train.outputs['training']}        \n",
    "    )    \n",
    "    \n",
    "    # deploy api serving as nuclio function\n",
    "    nuclio_api_serving = funcs['nuclio-api-serving'].deploy_step()\n",
    "    nuclio_api_serving.after(nuclio_face_prediction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-register-workflow\"></a>\n",
    "#### Register the Workflow\n",
    "\n",
    "Use the `set_workflow` MLRun project method to register your workflow with MLRun.\n",
    "The following code sets the `name` parameter to the selected workflow name (\"main\") and the `code` parameter to the name of the workflow file that is found in your project directory (**workflow.py**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the workflow file as \"main\"\n",
    "project.set_workflow('main', 'workflow.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.app-lab-3-b5874.iguazio-cd0.com/pipelines/#/experiments/details/838418b3-5eee-4e8d-8200-29ff5ac379bf\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.app-lab-3-b5874.iguazio-cd0.com/pipelines/#/runs/details/30148d2f-6761-49ad-af65-320e1ba01699\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-15 13:41:57,255 [info] Pipeline run id=30148d2f-6761-49ad-af65-320e1ba01699, check UI or DB for progress\n"
     ]
    }
   ],
   "source": [
    "run_id = project.run(\n",
    "    'main',\n",
    "    arguments={}, \n",
    "    \n",
    "    artifact_path=path.abspath(path.join('pipeline','{{workflow.uid}}'),\n",
    "    \n",
    "                              )\n",
    "    ,dirty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to [**client README.md**](../client/README.md) to clone client and generate images from your webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
